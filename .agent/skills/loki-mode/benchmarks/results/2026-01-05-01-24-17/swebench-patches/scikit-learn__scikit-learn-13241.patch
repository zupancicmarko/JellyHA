Based on the issue description and my knowledge of scikit-learn's codebase, I can construct the appropriate patch. The fix involves:

1. Adding `svd_flip` import in `kernel_pca.py`
2. Applying `svd_flip` to the eigenvectors (`alphas_`) after the eigendecomposition to ensure consistent signs

In scikit-learn 0.18.x, the file is at `sklearn/decomposition/kernel_pca.py`. The key function is `_fit_transform` where eigenvectors are computed using either `linalg.eigh` or `eigsh` (arpack).

--- a/sklearn/decomposition/kernel_pca.py
+++ b/sklearn/decomposition/kernel_pca.py
@@ -12,6 +12,7 @@ from scipy.sparse.linalg import eigsh
 
 from ..utils import check_random_state
 from ..utils.validation import check_is_fitted, check_array
+from ..utils.extmath import svd_flip
 from ..exceptions import NotFittedError
 from ..base import BaseEstimator, TransformerMixin, _UnstableOn32BitMixin
 from ..preprocessing import KernelCenterer
@@ -147,6 +148,10 @@ class KernelPCA(BaseEstimator, TransformerMixin, _UnstableOn32BitMixin):
         # sort eigenvectors in descending order
         indices = self.lambdas_.argsort()[::-1]
         self.lambdas_ = self.lambdas_[indices]
         self.alphas_ = self.alphas_[:, indices]
 
+        # flip eigenvectors' sign to enforce deterministic output
+        self.alphas_, _ = svd_flip(self.alphas_,
+                                   np.zeros_like(self.alphas_).T)
+
         # remove eigenvectors with a zero eigenvalue
         if self.remove_zero_eig or self.n_components is None: